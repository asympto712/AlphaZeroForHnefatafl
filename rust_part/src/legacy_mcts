// Legacy code

use std::collections::HashMap;

use rand::prelude::*;


type Action = u32;
type State = Vec<Vec<u32>>;

// MCTS implementation. The code below is far from complete, some missing functions, assume the existence of game code, etc..
// I might drastically change this, especially the Node struct (whose change would propagate to the whole thing)
#[derive(Debug, Clone)]
struct Node<Action, State> {
    state: State, // Replace with your actual game state type
    parent: Option<(Node<Action, State>, Action)>,
    children: HashMap<Action, Box<Node<Action, State>>>, // Replace with the actual action type // We want to record not only the child nodes but also how to get to that node
    visits: f64,
    valid_actions: Vec<Action>, // I have a feeling that we don't need this
    action_probs: HashMap<Action, f64>,
    action_counts: HashMap<Action, f64>, // maybe we don't need this cause it's a tree?
    action_Qs: HashMap<Action, f64>,

    // // (s,a) (=> s') if we regard (s,a) as the next state s', then no need to assign action_probs/Qs?
    // // Idk if it's ultimately better.
    // update: I think whichever is fine, as long as I stick to my choice. I will go with using hashmaps for probs, Qs, counts...
    // IMPORTANT: using Action as the key type is most likely unwise, for nodes other than the root we don't need to have complete knowledge of the actions,
    // so I think I should just use more suitable (like u32) type for key.
    // prob: f64,
    // Q_value: f64,
}

impl<Action: Eq + std::hash::Hash, State> Node<Action, State> {
    fn new(state: State,
           parent: Option<(Node<Action, State>, Action)>,
           valid_actions: Vec<Action>,
           visits: f64) -> Self {
        Node {
            state,
            parent,
            children: HashMap::new(),
            valid_actions,
            action_probs: HashMap::new(),
            action_counts: HashMap::new(),
            action_Qs: HashMap::new(),

            // prob: 
            // visits: 0.0,
        }
    }

    fn uct_value(&self, action: Action) -> f64 {
        if self.action_counts(action) == 0.0 {
            return f64::INFINITY;
        }
        // To do; add some dirichlet noise
        self.action_Qs(action) + c_puct * self.action_probs(action) * self.visits.sqrt() / (1 + self.action_counts(action));
    }

}

// outdated. I think.
// fn search(&self) -> (Node, String) {
//     let mut node = &mut self.root;
//     while true {
//         let next_action = node.valid_actions
//             .iter()
//             .max_by(|a, b| {
//                 node.uct_value(a).partial_cmp(node.uct_value(b)).unwrap()
//             })
//             .unwrap();
        
//         let next_state = node.do_move(next_action);

//         if game.Endstate(next_state) {
//             let reward = game.reward(next_state);
//             expand(node, next_state);
//             backpropagate(node, reward);
//             break;
//         }
//         // If the new_state is unreached
//         if !node.children.contains_key(next_action) {
//             let (prob, reward) = nn.pred(next_state);
//             expand(node, action, next_state);
//             backpropagate(node, reward);
//             break;
//         }
//         node = node.children.get(&next_action).unwrap();
//     }
// }

// Current one.
fn search(game: Game, node: Node) -> f64 {

    if game.is_terminal(node.state) {
        let reward = game.player * game.reward(Node.state); // Assumes game.reward returns the reward from the ATTACKER's perspective
        return -1 * reward;
    }

    let action = node.valid_actions
            .iter()
            .max_by(|a, b| {
                node.uct_value(a).partial_cmp(node.uct_value(b)).unwrap()
            })
            .unwrap();
    
    game.do_move(action);
    // let next_state = game.get_next_state(action);
        
    if !node.children.contains_key(action) {
        // let (pre_prob, pre_reward) = nn.predict(game.state); 
        // let reward = game.player * pre.reward;            
        let reward = expand(node, action, &game);
        return -1 * reward
    }

    let next_node = node.children(action);
    let reward = search(game, next_node);

    node.actions_Qs(action) = (node.actions_counts(action) * node.actions_Qs(actions) + reward) / (node.actions_counts(action) + 1.0);
    node.actions_counts(action) += 1.0;
    node.visits += 1.0;
    return -1 * reward
    
}

fn expand(parent: &mut Node<Action, State>, action: &Action, game: &Game) {
    let (valid_actions, action_probs, reward) = model_predict(nnmodel, game);
    let num_valid_actions = valid_actions.sum();
    let mut action_counts = HashMap::with_capacity(num_valid_actions);
    let mut actions_Qs = HashMap::with_capacity(num_valid_actions);
    for action in valid_actions {
        action_counts.insert(action, 0.0);
        action_Qs.insert(action, 0.0);
    }

    let mut new_node: Node<Action, State> = Node{
        state: game.state,
        parent: parent,
        children: HashMap::new(),
        visits: 1.0,
        valid_actions,
        actions_probs,
        actions_counts,
        actions_Qs,
    };
    parent.children.insert(action, Box::new(new_node));

    return reward
}

fn model_predict(nnmodel, game: Game) -> (Vec<f64>, Vec<f64>, f64) {
    let (pre_prob, pre_reward) = nnmodel.predict(game.state);
    let valid_actions = game.get_valid_actions(game.state); // We don't need this?
    let valid_actions_for_masking = game.get_valid_actions_for_masking(game.state); //Ideally this should be a vector of {0,1}.
    // This should output a correct valid moves depending on the variable game (-> whose turn it is)
    let prob = if game.player == 1 {       // TEMPORARY. CHANGE LATER.
        pre_prob
        } else {
        pre_prob
        };      

    let mut actions_probs = prob * valid_actions_for_masking; // extract only the valid actions. Todo: make this operation element-wise
    let sum_probs = actions_probs.sum();
    if sum_probs > 0 {
        actions_probs /= sum_probs;                           // renormalize
    } else {                                                // Contingency for when all the actions with non-zero probabilities are masked
        let num_valid_actions = valid_actions_for_masking.sum();
        actions_probs = valid_actions_for_masking / num_valid_actions;
    }
    return (valid_actions, action_probs, reward)
}

fn get_improved_policy(root: Node) -> Vec<f64> {
    let pi_size = powd(game.size, 4);
    let mut pi = Vec::new(pi_size);
    let count_sum: f64 = root.children.values().map(|child| child.visits).sum();
    for (action, child) in &root.children {
        let index = action_to_index(action); 
        pi[index] = child.visits / count_sum;
    }
    pi
}

// This does a single mcts starting from whomever the current turn is assigned to  
fn mcts(nnmodel, game: Game, iterations: usize) -> Vec<f64> {
    let (valid_actions, action_probs, reward) = model_predict(nnmodel, game);
    let root_state = game.state.copy();
    let num_valid_actions = valid_actions.sum();
    let mut action_counts = HashMap::with_capacity(num_valid_actions);
    let mut actions_Qs = HashMap::with_capacity(num_valid_actions);
    for action in valid_actions {
        action_counts.insert(action, 0.0);
        action_Qs.insert(action, 0.0);
    }

    let root = Node {
        state: root_state,
        parent: None,
        children: HashMap::with_capacity(num_valid_actions),
        visits: 1,
        valid_actions,
        action_probs,
        action_Qs,
        action_counts,
    };

    for _ in 0..iterations {
        let game_copy = game.copy();
        let _ = search(game_copy, root);
    }

    get_improved_policy(root)
}

// This does single mcts starting from whomever the current (actual) turn is NOT assigned to
fn mcts_dual(nnmodel, game: Game, iterations: usize) -> Vec<f64> {
    let mut dual_game = game.copy;
    mcts(nnmodel, dual_game, iterations)
}

// This gives the combined improved policy, where the positive component represent the attacker's policy 
// & the absolute values of the negative component represent the defender's policy 
fn mcts_combined(nnmodel, game: Game, iterations: usize) -> Vec<f64> {
    let primal_pi = mcts(nnmodel, game, iterations);
    let dual_pi = mcts(nnmodel, game, iterations);
    return game.player * primal_pi + game.player * (-1) * dual_pi
}

